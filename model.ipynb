{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.model import T5HeadWithValueModel\n",
    "from transformers import T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5HeadWithValueModel were not initialized from the model checkpoint at t5-base and are newly initialized: ['v_head.summary.weight', 'v_head.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = T5HeadWithValueModel.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "    \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ids = torch.argmax(outputs.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studies --> tensor([1]) </s>\n",
      "show --> tensor([1]) </s>\n",
      "that --> tensor([1]) </s>\n",
      "</s> --> tensor([1]) </s>\n"
     ]
    }
   ],
   "source": [
    "for i in range(decoder_input_ids.shape[1]):\n",
    "    current_id = tokenizer.decode(decoder_input_ids[:, i])\n",
    "    next_id = tokenizer.decode(pred_ids[:, i], skip_special_tokens=False)\n",
    "    print(current_id, '-->',pred_ids[:, i], next_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0030, -0.0015, -0.0038,  0.0014]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   644,  4598,   229, 19250,     5,     1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> Das Haus ist wunderbar.</s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_txt_1 = \"My most favourite movie is Transformers.\"\n",
    "query_txt_2 = \"I eat an apple.\"\n",
    "queries_txt = [\"translate English to French: \"+q for q in [query_txt_1, query_txt_2]]\n",
    "\n",
    "queries = tokenizer(queries_txt, return_tensors=\"pt\", padding=\"max_length\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jader/portuguese-paraphrasing/model.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jader/portuguese-paraphrasing/model.ipynb#ch0000018vscode-remote?line=0'>1</a>\u001b[0m responses \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrespond_to_batch(queries)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jader/portuguese-paraphrasing/model.ipynb#ch0000018vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m responses:\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/jader/portuguese-paraphrasing/model.ipynb#ch0000018vscode-remote?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mdecode(r))\n",
      "File \u001b[0;32m~/portuguese-paraphrasing/utils/model.py:215\u001b[0m, in \u001b[0;36mT5HeadWithValueModel.respond_to_batch\u001b[0;34m(self, queries, txt_len, top_k, top_p)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=211'>212</a>\u001b[0m decoder_input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(queries\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=212'>213</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(txt_len):\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=213'>214</a>\u001b[0m     \u001b[39m# Get Logits\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=214'>215</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(input_ids\u001b[39m=\u001b[39;49minput_ids, decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids)\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=215'>216</a>\u001b[0m     next_token_logits \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m][:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=216'>217</a>\u001b[0m     next_token_logits \u001b[39m=\u001b[39m top_k_top_p_filtering(next_token_logits, top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p)\n",
      "File \u001b[0;32m~/portuguese-paraphrasing/utils/model.py:118\u001b[0m, in \u001b[0;36mT5HeadWithValueModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=114'>115</a>\u001b[0m \u001b[39m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=115'>116</a>\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=116'>117</a>\u001b[0m     \u001b[39m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=117'>118</a>\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=118'>119</a>\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=119'>120</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=120'>121</a>\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=121'>122</a>\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=122'>123</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=123'>124</a>\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=124'>125</a>\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=125'>126</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=126'>127</a>\u001b[0m \u001b[39melif\u001b[39;00m return_dict \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=127'>128</a>\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=128'>129</a>\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=129'>130</a>\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=130'>131</a>\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/utils/model.py?line=131'>132</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:1033\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1019'>1020</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1020'>1021</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1021'>1022</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1029'>1030</a>\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1030'>1031</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1031'>1032</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1032'>1033</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1033'>1034</a>\u001b[0m         hidden_states,\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1034'>1035</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1035'>1036</a>\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1036'>1037</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1037'>1038</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1038'>1039</a>\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1039'>1040</a>\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1040'>1041</a>\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1041'>1042</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1042'>1043</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1043'>1044</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1044'>1045</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1046'>1047</a>\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1047'>1048</a>\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:720\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=716'>717</a>\u001b[0m     attention_outputs \u001b[39m=\u001b[39m attention_outputs \u001b[39m+\u001b[39m cross_attention_outputs[\u001b[39m2\u001b[39m:]\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=718'>719</a>\u001b[0m \u001b[39m# Apply Feed Forward layer\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=719'>720</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m](hidden_states)\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=721'>722</a>\u001b[0m \u001b[39m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=722'>723</a>\u001b[0m \u001b[39mif\u001b[39;00m hidden_states\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16 \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39misinf(hidden_states)\u001b[39m.\u001b[39many():\n",
      "File \u001b[0;32m~/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:328\u001b[0m, in \u001b[0;36mT5LayerFF.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=325'>326</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=326'>327</a>\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=327'>328</a>\u001b[0m     forwarded_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDenseReluDense(forwarded_states)\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=328'>329</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(forwarded_states)\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=329'>330</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py:286\u001b[0m, in \u001b[0;36mT5DenseReluDense.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=284'>285</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m--> <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=285'>286</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwi(hidden_states)\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=286'>287</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(hidden_states)\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py?line=287'>288</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m~/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/jader/portuguese-paraphrasing/venv/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "responses = model.respond_to_batch(queries)\n",
    "for r in responses:\n",
    "    print(tokenizer.decode(r))\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon film le plus préféré est Transformers.\n",
      "------\n",
      "Je mange une pomme.\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "decoding_config = {\n",
    "    \"temperature\": 2.0,\n",
    "    \"top_k\": 10,\n",
    "    \"top_p\": 0.7,\n",
    "    \"typical_p\": 0.2\n",
    "}\n",
    "responses = model.generate(queries, **decoding_config)\n",
    "for r in responses:\n",
    "    print(tokenizer.decode(r, skip_special_tokens=True))\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  2963,   814,    90,   303, 22157,   259, 31220,     7,     5,\n",
       "             1],\n",
       "        [    0,  1022,   388,   397,   245, 26234,     5,     1,     0,     0,\n",
       "             0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mon film le plus préféré est Transformers.', 'Je mange une pomme.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(responses, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ebfffb3a24fda823f036facdfb39bf282cbfee0e7ae18fbb3d02881c1a2e41bd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
